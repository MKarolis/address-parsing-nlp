{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff206dfb",
   "metadata": {},
   "source": [
    "# Improving Named Entity Recognition address parsing\n",
    "Data & AI course, UC Leuven, 2021 Fall\n",
    "### Project supervisors\n",
    "- Tom Magerman\n",
    "- Aimée Lynn Backiel\n",
    "\n",
    "### Project team (Group 4)\n",
    "- Karolis Medekša\n",
    "- Pedro Teixeira Palma Rosa\n",
    "- Hysa Mello de Alcântara\n",
    "- Josep Jacob Chetrit Valdepeñas\n",
    "\n",
    "## Goals\n",
    "The goal of the assignment is to try and improve the existing solution for parsing addresses using NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea31c2a7",
   "metadata": {},
   "source": [
    "# Existing improvements\n",
    "### The following improvements were already implemented with the first draft of the solution, which, in our opinion, are noteworthy:\n",
    "- Fixing mistakes in the training dataset by hand (there might still be mistakes in the validation set)\n",
    "- Fixing conflicting entity errors when training the model\n",
    "- Pre-parsing the data so that tokenizer can recognize all tokens\n",
    "- Improving the algorithm to handle overlapping entities\n",
    "- Fine-tuning the `drop` criteria and iteration count\n",
    "\n",
    "For more information how these improvements were implemented please consult the `DOCUMENTATION` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c93a1",
   "metadata": {},
   "source": [
    "# Evaluating NER model performance by country\n",
    "One thing to look into with the model is how well does it perform with different regions. Postal codes are different among different countries, moreover, cities and regions can differ greatly among regions.\n",
    "\n",
    "To conduct the experiment, we first train a baseline model as the result of the initial exercise (refer to `DOCUMENTATION` for more information about it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb966fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "## Define utility functions:\n",
    "\n",
    "def read_DataFrame_from_excel(filename: str, numberOfRows: int = None):\n",
    "    return pd.read_excel(filename, nrows = numberOfRows, keep_default_na=False)\n",
    "\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame):\n",
    "    for col in data.columns:\n",
    "        data[col] = data.apply(lambda row: re.sub(r'([^\\s])([,;])([^\\s])', r'\\1\\2 \\3', str(row[col])), axis=1)\n",
    "\n",
    "\n",
    "def entities_overlap(entry):\n",
    "    entities = entry[1]['entities']\n",
    "    for first in entities:\n",
    "        for second in entities:\n",
    "            if (first == second): continue\n",
    "            if (first[0] < second[0] and first[1] > second[0]) or (first[0] > second[0] and first[1] < second[0]) or (first[0]==second[0] or first[1]==second[1]):\n",
    "                print('Entities {} and {} overlap in \"{}\"'.format(first, second, entry[0]))\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_entity_list(entry: dict, adr: str):\n",
    "    address = str(adr)\n",
    "    entities: list = []\n",
    "    present_tokens = filter(lambda item: item[0] in TOKEN_TYPES and item[1] and str(item[1]).strip(), entry.items())\n",
    "\n",
    "    ## tokens to retry matching\n",
    "    retry_tokens: set = set()\n",
    "\n",
    "    for item in present_tokens:\n",
    "        token_value = str(item[1]).strip()\n",
    "        match = re.search(re.escape(token_value), address)\n",
    "        if match:\n",
    "            # If multiple occurences can be matched, save the token to be matched later\n",
    "            if (len(re.findall(re.escape(token_value), address)) > 1):\n",
    "                retry_tokens.add((token_value, item[0]))\n",
    "                continue\n",
    "            span = match.span()\n",
    "            entities.append((span[0], span[1], item[0]))\n",
    "            # Replace matched entity with symbols, so that parts of it cannot be matched again\n",
    "            address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "        else:\n",
    "            # Try and resolve multiple tokens separated by ';'\n",
    "            split_items = map(lambda token: token.strip(), token_value.split(';'))\n",
    "            for token in split_items:\n",
    "                split_match = re.search(re.escape(token), address)\n",
    "                if split_match:\n",
    "                    # If multiple occurences can be matched, save the token to be matched later\n",
    "                    if (len(re.findall(re.escape(token), address)) > 1):\n",
    "                        retry_tokens.add((token, item[0]))\n",
    "                        continue\n",
    "                    span = split_match.span()\n",
    "                    entities.append((span[0], span[1], item[0]))\n",
    "                    # Replace matched entity with symbols, so that parts of it cannot be matched again\n",
    "                    address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "                else:\n",
    "                    print('WARNING: could not find token \"{}\" in address \"{}\"'.format(token, adr))\n",
    "    \n",
    "    # Try and match previously marked tokens, now that single-match entities were eliminated\n",
    "    for token, tkn_type in retry_tokens:\n",
    "        token_value = str(token).strip()\n",
    "        match = re.search(re.escape(token_value), address)\n",
    "        if match:\n",
    "            span = match.span()\n",
    "            entities.append((span[0], span[1], tkn_type))\n",
    "            address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "        else:\n",
    "            print('WARNING: could not find token \"{}\" in address \"{}\"'.format(token, adr))\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def map_to_training_entry(entry: dict):\n",
    "    address = entry['person_address']\n",
    "    return (address, {\n",
    "        'entities': get_entity_list(entry, address)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd3dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train entries: 799 | test entries: 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "TOKEN_TYPES: set = {'co', 'building', 'street', 'nr', 'area', 'postal', 'city', 'region', 'country'}\n",
    "\n",
    "raw_data: pd.DataFrame = read_DataFrame_from_excel('../files/training_data_fixed.xlsx', 999)\n",
    "preprocess_data(raw_data)\n",
    "\n",
    "train_data = list(\n",
    "    map(map_to_training_entry, raw_data.to_dict('records'))\n",
    ")\n",
    "train_data = list(filter(lambda entry: not entities_overlap(entry), train_data))\n",
    "\n",
    "train_sample, test_sample = train_test_split(\n",
    "    train_data, test_size = 0.2, random_state = 420\n",
    ")\n",
    "print('train entries: {} | test entries: {}'.format(len(train_sample), len(test_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c32154f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\envs\\nlp-deps\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"10100 Bay Area Boulevard~Pasadena~Texas~77507 US\" with entities \"[(6, 24, 'street'), (0, 5, 'nr'), (40, 45, 'postal...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\Home\\anaconda3\\envs\\nlp-deps\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"c/o Hitachi Research Laboratory, HITACHI LTD., 1-1...\" with entities \"[(33, 45, 'co'), (5, 31, 'building'), (69, 88, 'st...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 | Losses: {'ner': 3977.276416558074}\n",
      "Iteration: 1 | Losses: {'ner': 3580.2742863950175}\n",
      "Iteration: 2 | Losses: {'ner': 3282.5660284712194}\n",
      "Iteration: 3 | Losses: {'ner': 3045.0924848197546}\n",
      "Iteration: 4 | Losses: {'ner': 2925.8011155040053}\n",
      "Iteration: 5 | Losses: {'ner': 2792.9772251557188}\n",
      "Iteration: 6 | Losses: {'ner': 2665.614183408595}\n",
      "Iteration: 7 | Losses: {'ner': 2623.1970171048315}\n",
      "Iteration: 8 | Losses: {'ner': 2583.3433715226765}\n",
      "Iteration: 9 | Losses: {'ner': 2490.063479144209}\n",
      "Iteration: 10 | Losses: {'ner': 2361.784224360041}\n",
      "Iteration: 11 | Losses: {'ner': 2312.013685919401}\n",
      "Iteration: 12 | Losses: {'ner': 2199.3338798884456}\n",
      "Iteration: 13 | Losses: {'ner': 2244.988300194779}\n",
      "Iteration: 14 | Losses: {'ner': 2119.654617292578}\n",
      "Iteration: 15 | Losses: {'ner': 2089.4424322877303}\n",
      "Iteration: 16 | Losses: {'ner': 2079.1763659212284}\n",
      "Iteration: 17 | Losses: {'ner': 1929.159082558892}\n",
      "Iteration: 18 | Losses: {'ner': 1862.7613717149006}\n",
      "Iteration: 19 | Losses: {'ner': 1839.7585401293888}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "for token in TOKEN_TYPES:\n",
    "    ner.add_label(token)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "for itn in range(20):\n",
    "    random.shuffle(train_sample)\n",
    "    losses = {}\n",
    "\n",
    "    batches = minibatch(train_sample, size=compounding(4, 32, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "            texts,  \n",
    "            annotations,  \n",
    "            drop=0.5,  \n",
    "            sgd=optimizer,\n",
    "            losses=losses)\n",
    "    print('Iteration: {} | Losses: {}'.format(itn, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "097a524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def results_per_entity_to_df(res: dict):\n",
    "    columns = ['Token', 'Precision', 'Recall', 'F1 score']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    total = pd.concat(\n",
    "        [pd.DataFrame([['Total', res['ents_p'], res['ents_r'], res['ents_f']]], columns=columns)]\n",
    "        , ignore_index=True\n",
    "    )\n",
    "    per_entity = pd.concat(\n",
    "        [pd.DataFrame([\n",
    "            [token, \n",
    "             res['ents_per_type'][token]['p'], \n",
    "             res['ents_per_type'][token]['r'], \n",
    "             res['ents_per_type'][token]['f']]\n",
    "        ], columns=columns) for token in TOKEN_TYPES], ignore_index=True\n",
    "    )\n",
    "    return pd.concat([per_entity, total], ignore_index=True)\n",
    "\n",
    "\n",
    "def map_to_evaluation_model(entry: tuple):\n",
    "    return (entry[0], entry[1]['entities'])\n",
    "\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot)\n",
    "        pred_value = ner_model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores\n",
    "\n",
    "train_results = evaluate(nlp, map(map_to_evaluation_model, train_sample))\n",
    "test_results = evaluate(nlp, map(map_to_evaluation_model, test_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af8f31ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- GENERAL: Results on train data ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>76.190476</td>\n",
       "      <td>48.484848</td>\n",
       "      <td>59.259259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>94.900850</td>\n",
       "      <td>98.820059</td>\n",
       "      <td>96.820809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>82.386364</td>\n",
       "      <td>65.315315</td>\n",
       "      <td>72.864322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>89.164491</td>\n",
       "      <td>98.414986</td>\n",
       "      <td>93.561644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>62.616822</td>\n",
       "      <td>65.048544</td>\n",
       "      <td>63.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>78.009259</td>\n",
       "      <td>86.410256</td>\n",
       "      <td>81.995134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>92.164179</td>\n",
       "      <td>85.172414</td>\n",
       "      <td>88.530466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>27.027027</td>\n",
       "      <td>37.735849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>88.050314</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>90.614887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>86.130206</td>\n",
       "      <td>87.116564</td>\n",
       "      <td>86.620577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- GENERAL: Results on test data ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>92.391304</td>\n",
       "      <td>92.391304</td>\n",
       "      <td>92.391304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>41.463415</td>\n",
       "      <td>28.813559</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>71.090047</td>\n",
       "      <td>84.745763</td>\n",
       "      <td>77.319588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>47.826087</td>\n",
       "      <td>45.833333</td>\n",
       "      <td>46.808511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>62.264151</td>\n",
       "      <td>68.041237</td>\n",
       "      <td>65.024631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>84.615385</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>83.018868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>72.289157</td>\n",
       "      <td>77.922078</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>70.962733</td>\n",
       "      <td>73.828756</td>\n",
       "      <td>72.367379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "print('---- GENERAL: Results on train data ----')\n",
    "display(HTML(results_per_entity_to_df(train_results).to_html(index=False)))\n",
    "print('---- GENERAL: Results on test data ----')\n",
    "display(HTML(results_per_entity_to_df(test_results).to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83600ae5",
   "metadata": {},
   "source": [
    "The precision/recall of attributes postal, city, street and house number are the most important, so we'll look into optimizing them. First, let's check how many each country's addresses are there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b3483b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person_ctry_code\n",
       "US    347\n",
       "JP    198\n",
       "DE     89\n",
       "FR     48\n",
       "KR     43\n",
       "GB     37\n",
       "CN     27\n",
       "TW     18\n",
       "IT     16\n",
       "CA     16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.groupby(['person_ctry_code']).size().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b52b43",
   "metadata": {},
   "source": [
    "We can also check how accurate the predictions are with each country's addresses, including both seen and unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee3ba572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correctness_by_country(country_code: str, frame: pd.DataFrame):\n",
    "    filtered = frame[frame['person_ctry_code'] == country_code]\n",
    "    mapped = list(\n",
    "        map(map_to_training_entry, filtered.to_dict('records'))\n",
    "    )\n",
    "    results = evaluate(nlp, map(map_to_evaluation_model, mapped))\n",
    "    print('---- Results on {} addresses ----'.format(country_code))\n",
    "    display(HTML(results_per_entity_to_df(results).to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c1a84c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Results on US addresses ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp/ipykernel_10384/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"10100 Bay Area Boulevard~Pasadena~Texas~77507 US\" with entities \"[(6, 24, 'street'), (0, 5, 'nr'), (40, 45, 'postal...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>95.967742</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.942387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>80.645161</td>\n",
       "      <td>40.983607</td>\n",
       "      <td>54.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>89.329268</td>\n",
       "      <td>97.019868</td>\n",
       "      <td>93.015873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>85.185185</td>\n",
       "      <td>94.262295</td>\n",
       "      <td>89.494163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>98.260870</td>\n",
       "      <td>93.388430</td>\n",
       "      <td>95.762712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>54.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>95.416667</td>\n",
       "      <td>98.283262</td>\n",
       "      <td>96.828753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>91.374122</td>\n",
       "      <td>92.299899</td>\n",
       "      <td>91.834677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_correctness_by_country('US', raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99ee71fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp/ipykernel_10384/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"50-6, Ipponmatsu, Menjyo, Yamato-cho, Ichinomiya-s...\" with entities \"[(6, 16, 'street'), (0, 4, 'nr'), (18, 24, 'area')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n",
      "C:\\Users\\Home\\AppData\\Local\\Temp/ipykernel_10384/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"c/o Hitachi Research Laboratory, HITACHI LTD., 1-1...\" with entities \"[(33, 45, 'co'), (5, 31, 'building'), (69, 88, 'st...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Results on JP addresses ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>93.150685</td>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>73.469388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>89.534884</td>\n",
       "      <td>82.795699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>60.869565</td>\n",
       "      <td>60.869565</td>\n",
       "      <td>60.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>54.615385</td>\n",
       "      <td>62.280702</td>\n",
       "      <td>58.196721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>21.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>69.354839</td>\n",
       "      <td>70.491803</td>\n",
       "      <td>69.918699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>69.277108</td>\n",
       "      <td>71.651090</td>\n",
       "      <td>70.444104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_correctness_by_country('JP', raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d5b7820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Results on DE addresses ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>28.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.989899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>61.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>92.857143</td>\n",
       "      <td>96.296296</td>\n",
       "      <td>94.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>95.744681</td>\n",
       "      <td>88.235294</td>\n",
       "      <td>91.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>91.836735</td>\n",
       "      <td>91.836735</td>\n",
       "      <td>91.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>91.129032</td>\n",
       "      <td>91.497976</td>\n",
       "      <td>91.313131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_correctness_by_country('DE', raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9eda9f",
   "metadata": {},
   "source": [
    "Training the model without Japanese addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68e38f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train entries: 640 | test entries: 161\n"
     ]
    }
   ],
   "source": [
    "train_data = list(\n",
    "    map(map_to_training_entry, raw_data[raw_data['person_ctry_code'] != 'JP'].to_dict('records'))\n",
    ")\n",
    "train_data = list(filter(lambda entry: not entities_overlap(entry), train_data))\n",
    "\n",
    "train_sample, test_sample = train_test_split(\n",
    "    train_data, test_size = 0.2, random_state = 420\n",
    ")\n",
    "print('train entries: {} | test entries: {}'.format(len(train_sample), len(test_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "689e4735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\envs\\nlp-deps\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"10100 Bay Area Boulevard~Pasadena~Texas~77507 US\" with entities \"[(6, 24, 'street'), (0, 5, 'nr'), (40, 45, 'postal...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 | Losses: {'ner': 2850.7694831148256}\n",
      "Iteration: 1 | Losses: {'ner': 2337.574111491176}\n",
      "Iteration: 2 | Losses: {'ner': 2102.10072309518}\n",
      "Iteration: 3 | Losses: {'ner': 2017.526945453805}\n",
      "Iteration: 4 | Losses: {'ner': 1961.4871999486684}\n",
      "Iteration: 5 | Losses: {'ner': 1925.7118151357736}\n",
      "Iteration: 6 | Losses: {'ner': 1802.8459183709767}\n",
      "Iteration: 7 | Losses: {'ner': 1725.1936288149025}\n",
      "Iteration: 8 | Losses: {'ner': 1640.6068277296922}\n",
      "Iteration: 9 | Losses: {'ner': 1646.875747922958}\n",
      "Iteration: 10 | Losses: {'ner': 1653.0190410545306}\n",
      "Iteration: 11 | Losses: {'ner': 1544.3840787018908}\n",
      "Iteration: 12 | Losses: {'ner': 1507.046106418851}\n",
      "Iteration: 13 | Losses: {'ner': 1433.7361705189355}\n",
      "Iteration: 14 | Losses: {'ner': 1476.5068128975824}\n",
      "Iteration: 15 | Losses: {'ner': 1417.6282565029799}\n",
      "Iteration: 16 | Losses: {'ner': 1358.5674984634775}\n",
      "Iteration: 17 | Losses: {'ner': 1301.4037850109555}\n",
      "Iteration: 18 | Losses: {'ner': 1303.6894518329257}\n",
      "Iteration: 19 | Losses: {'ner': 1258.9307017848275}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "for token in TOKEN_TYPES:\n",
    "    ner.add_label(token)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "for itn in range(20):\n",
    "    random.shuffle(train_sample)\n",
    "    losses = {}\n",
    "\n",
    "    batches = minibatch(train_sample, size=compounding(4, 32, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "            texts,  \n",
    "            annotations,  \n",
    "            drop=0.5,  \n",
    "            sgd=optimizer,\n",
    "            losses=losses)\n",
    "    print('Iteration: {} | Losses: {}'.format(itn, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f438a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp/ipykernel_10384/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"10100 Bay Area Boulevard~Pasadena~Texas~77507 US\" with entities \"[(6, 24, 'street'), (0, 5, 'nr'), (40, 45, 'postal...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Without JAPAN: Results on train data ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>80.952381</td>\n",
       "      <td>60.714286</td>\n",
       "      <td>69.387755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>97.250859</td>\n",
       "      <td>98.263889</td>\n",
       "      <td>97.754750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>87.096774</td>\n",
       "      <td>55.102041</td>\n",
       "      <td>67.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>91.074380</td>\n",
       "      <td>98.392857</td>\n",
       "      <td>94.592275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>66.101695</td>\n",
       "      <td>73.584906</td>\n",
       "      <td>69.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>85.119048</td>\n",
       "      <td>94.389439</td>\n",
       "      <td>89.514867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>96.168582</td>\n",
       "      <td>92.279412</td>\n",
       "      <td>94.183865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>43.956044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>90.650407</td>\n",
       "      <td>90.283401</td>\n",
       "      <td>90.466531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>89.887064</td>\n",
       "      <td>89.656938</td>\n",
       "      <td>89.771853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Without JAPAN: Results on test data ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>94.029851</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>91.970803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>17.647059</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>77.215190</td>\n",
       "      <td>87.769784</td>\n",
       "      <td>82.154882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>62.244898</td>\n",
       "      <td>87.142857</td>\n",
       "      <td>72.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>77.631579</td>\n",
       "      <td>85.507246</td>\n",
       "      <td>81.379310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>75.362319</td>\n",
       "      <td>80.620155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>74.743326</td>\n",
       "      <td>77.611940</td>\n",
       "      <td>76.150628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = evaluate(nlp, map(map_to_evaluation_model, train_sample))\n",
    "test_results = evaluate(nlp, map(map_to_evaluation_model, test_sample))\n",
    "\n",
    "print('---- Without JAPAN: Results on train data ----')\n",
    "display(HTML(results_per_entity_to_df(train_results).to_html(index=False)))\n",
    "print('---- Without JAPAN: Results on test data ----')\n",
    "display(HTML(results_per_entity_to_df(test_results).to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "146d1bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp/ipykernel_10384/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"50-6, Ipponmatsu, Menjyo, Yamato-cho, Ichinomiya-s...\" with entities \"[(6, 16, 'street'), (0, 4, 'nr'), (18, 24, 'area')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n",
      "C:\\Users\\Home\\AppData\\Local\\Temp/ipykernel_10384/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"c/o Hitachi Research Laboratory, HITACHI LTD., 1-1...\" with entities \"[(33, 45, 'co'), (5, 31, 'building'), (69, 88, 'st...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Results on JP addresses ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>79.012346</td>\n",
       "      <td>87.671233</td>\n",
       "      <td>83.116883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>29.629630</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>20.779221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>50.367647</td>\n",
       "      <td>79.651163</td>\n",
       "      <td>61.711712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>49.122807</td>\n",
       "      <td>40.579710</td>\n",
       "      <td>44.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>26.277372</td>\n",
       "      <td>31.578947</td>\n",
       "      <td>28.685259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>21.621622</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>30.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>1.639344</td>\n",
       "      <td>2.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>43.145743</td>\n",
       "      <td>46.573209</td>\n",
       "      <td>44.794007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_correctness_by_country('JP', raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e3dbb",
   "metadata": {},
   "source": [
    "Only Japanese addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "126e347e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train entries: 158 | test entries: 40\n"
     ]
    }
   ],
   "source": [
    "train_data = list(\n",
    "    map(map_to_training_entry, raw_data[raw_data['person_ctry_code'] == 'JP'].to_dict('records'))\n",
    ")\n",
    "train_data = list(filter(lambda entry: not entities_overlap(entry), train_data))\n",
    "\n",
    "train_sample, test_sample = train_test_split(\n",
    "    train_data, test_size = 0.2, random_state = 420\n",
    ")\n",
    "print('train entries: {} | test entries: {}'.format(len(train_sample), len(test_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e367a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\envs\\nlp-deps\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"c/o Hitachi Research Laboratory, HITACHI LTD., 1-1...\" with entities \"[(33, 45, 'co'), (5, 31, 'building'), (69, 88, 'st...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\Home\\anaconda3\\envs\\nlp-deps\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"50-6, Ipponmatsu, Menjyo, Yamato-cho, Ichinomiya-s...\" with entities \"[(6, 16, 'street'), (0, 4, 'nr'), (18, 24, 'area')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 | Losses: {'ner': 1296.288991034031}\n",
      "Iteration: 1 | Losses: {'ner': 1019.4307759591611}\n",
      "Iteration: 2 | Losses: {'ner': 1222.0193906500936}\n",
      "Iteration: 3 | Losses: {'ner': 1190.3264100253582}\n",
      "Iteration: 4 | Losses: {'ner': 1155.5151634812355}\n",
      "Iteration: 5 | Losses: {'ner': 1061.6250290572643}\n",
      "Iteration: 6 | Losses: {'ner': 1034.111961901188}\n",
      "Iteration: 7 | Losses: {'ner': 926.5234970450401}\n",
      "Iteration: 8 | Losses: {'ner': 908.6183833181858}\n",
      "Iteration: 9 | Losses: {'ner': 866.6283576494316}\n",
      "Iteration: 10 | Losses: {'ner': 817.3716110667738}\n",
      "Iteration: 11 | Losses: {'ner': 794.978878598311}\n",
      "Iteration: 12 | Losses: {'ner': 855.1516721062071}\n",
      "Iteration: 13 | Losses: {'ner': 792.2007579095662}\n",
      "Iteration: 14 | Losses: {'ner': 804.8770619569113}\n",
      "Iteration: 15 | Losses: {'ner': 717.5790040045977}\n",
      "Iteration: 16 | Losses: {'ner': 745.0605421150103}\n",
      "Iteration: 17 | Losses: {'ner': 754.5750753134489}\n",
      "Iteration: 18 | Losses: {'ner': 740.428767343983}\n",
      "Iteration: 19 | Losses: {'ner': 698.7350066629806}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "for token in TOKEN_TYPES:\n",
    "    ner.add_label(token)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "for itn in range(20):\n",
    "    random.shuffle(train_sample)\n",
    "    losses = {}\n",
    "\n",
    "    batches = minibatch(train_sample, size=compounding(4, 32, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "            texts,  \n",
    "            annotations,  \n",
    "            drop=0.5,  \n",
    "            sgd=optimizer,\n",
    "            losses=losses)\n",
    "    print('Iteration: {} | Losses: {}'.format(itn, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b47ab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp/ipykernel_10384/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"50-6, Ipponmatsu, Menjyo, Yamato-cho, Ichinomiya-s...\" with entities \"[(6, 16, 'street'), (0, 4, 'nr'), (18, 24, 'area')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n",
      "C:\\Users\\Home\\AppData\\Local\\Temp/ipykernel_10384/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"c/o Hitachi Research Laboratory, HITACHI LTD., 1-1...\" with entities \"[(33, 45, 'co'), (5, 31, 'building'), (69, 88, 'st...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Results on JP addresses ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>86.486486</td>\n",
       "      <td>87.671233</td>\n",
       "      <td>87.074830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>61.467890</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>64.114833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>72.300469</td>\n",
       "      <td>89.534884</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>55.223881</td>\n",
       "      <td>53.623188</td>\n",
       "      <td>54.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>54.700855</td>\n",
       "      <td>56.140351</td>\n",
       "      <td>55.411255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>22.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>40.983607</td>\n",
       "      <td>54.945055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>67.680000</td>\n",
       "      <td>65.887850</td>\n",
       "      <td>66.771902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_correctness_by_country('JP', raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2558488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
