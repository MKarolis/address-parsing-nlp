{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff206dfb",
   "metadata": {},
   "source": [
    "# Improving Named Entity Recognition address parsing\n",
    "Data & AI course, UC Leuven, 2021 Fall\n",
    "### Project supervisors\n",
    "- Tom Magerman\n",
    "- Aimée Lynn Backiel\n",
    "\n",
    "### Project team (Group 4)\n",
    "- Karolis Medekša\n",
    "- Pedro Teixeira Palma Rosa\n",
    "- Hysa Mello de Alcântara\n",
    "- Josep Jacob Chetrit Valdepeñas\n",
    "\n",
    "## Goals\n",
    "The goal of the assignment is to try and improve the existing solution for parsing addresses using NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea31c2a7",
   "metadata": {},
   "source": [
    "# Existing improvements\n",
    "### The following improvements were already implemented with the first draft of the solution, which, in our opinion, are noteworthy:\n",
    "- Fixing mistakes in the training dataset by hand (there might still be mistakes in the validation set)\n",
    "- Fixing conflicting entity errors when training the model\n",
    "- Pre-parsing the data so that tokenizer can recognize all tokens\n",
    "- Improving the algorithm to handle overlapping entities\n",
    "- Fine-tuning the `drop` criteria and iteration count\n",
    "\n",
    "For more information how these improvements were implemented please consult the `DOCUMENTATION` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c93a1",
   "metadata": {},
   "source": [
    "# Evaluating NER model performance by country\n",
    "One thing to look into with the model is how well does it perform with different regions. Postal codes are different among different countries, moreover, cities and regions can differ greatly among regions.\n",
    "\n",
    "To conduct the experiment, we first train a baseline model as the result of the initial exercise (refer to `DOCUMENTATION` for more information about it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bb966fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "## Define utility functions:\n",
    "\n",
    "def read_DataFrame_from_excel(filename: str, numberOfRows: int = None):\n",
    "    return pd.read_excel(filename, nrows = numberOfRows, keep_default_na=False)\n",
    "\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame):\n",
    "    for col in data.columns:\n",
    "        data[col] = data.apply(lambda row: re.sub(r'([^\\s])([,;])([^\\s])', r'\\1\\2 \\3', str(row[col])), axis=1)\n",
    "\n",
    "\n",
    "def entities_overlap(entry):\n",
    "    entities = entry[1]['entities']\n",
    "    for first in entities:\n",
    "        for second in entities:\n",
    "            if (first == second): continue\n",
    "            if (first[0] < second[0] and first[1] > second[0]) or (first[0] > second[0] and first[1] < second[0]) or (first[0]==second[0] or first[1]==second[1]):\n",
    "                print('Entities {} and {} overlap in \"{}\"'.format(first, second, entry[0]))\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_entity_list(entry: dict, adr: str):\n",
    "    address = str(adr)\n",
    "    entities: list = []\n",
    "    present_tokens = filter(lambda item: item[0] in TOKEN_TYPES and item[1] and str(item[1]).strip(), entry.items())\n",
    "\n",
    "    ## tokens to retry matching\n",
    "    retry_tokens: set = set()\n",
    "\n",
    "    for item in present_tokens:\n",
    "        token_value = str(item[1]).strip()\n",
    "        match = re.search(re.escape(token_value), address)\n",
    "        if match:\n",
    "            # If multiple occurences can be matched, save the token to be matched later\n",
    "            if (len(re.findall(re.escape(token_value), address)) > 1):\n",
    "                retry_tokens.add((token_value, item[0]))\n",
    "                continue\n",
    "            span = match.span()\n",
    "            entities.append((span[0], span[1], item[0]))\n",
    "            # Replace matched entity with symbols, so that parts of it cannot be matched again\n",
    "            address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "        else:\n",
    "            # Try and resolve multiple tokens separated by ';'\n",
    "            split_items = map(lambda token: token.strip(), token_value.split(';'))\n",
    "            for token in split_items:\n",
    "                split_match = re.search(re.escape(token), address)\n",
    "                if split_match:\n",
    "                    # If multiple occurences can be matched, save the token to be matched later\n",
    "                    if (len(re.findall(re.escape(token), address)) > 1):\n",
    "                        retry_tokens.add((token, item[0]))\n",
    "                        continue\n",
    "                    span = split_match.span()\n",
    "                    entities.append((span[0], span[1], item[0]))\n",
    "                    # Replace matched entity with symbols, so that parts of it cannot be matched again\n",
    "                    address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "                else:\n",
    "                    print('WARNING: could not find token \"{}\" in address \"{}\"'.format(token, adr))\n",
    "    \n",
    "    # Try and match previously marked tokens, now that single-match entities were eliminated\n",
    "    for token, tkn_type in retry_tokens:\n",
    "        token_value = str(token).strip()\n",
    "        match = re.search(re.escape(token_value), address)\n",
    "        if match:\n",
    "            span = match.span()\n",
    "            entities.append((span[0], span[1], tkn_type))\n",
    "            address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "        else:\n",
    "            print('WARNING: could not find token \"{}\" in address \"{}\"'.format(token, adr))\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def map_to_training_entry(entry: dict):\n",
    "    address = entry['person_address']\n",
    "    return (address, {\n",
    "        'entities': get_entity_list(entry, address)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcd3dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train entries: 799 | test entries: 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "TOKEN_TYPES: set = {'co', 'building', 'street', 'nr', 'area', 'postal', 'city', 'region', 'country'}\n",
    "\n",
    "raw_data: pd.DataFrame = read_DataFrame_from_excel('../files/training_data_fixed.xlsx', 999)\n",
    "preprocess_data(raw_data)\n",
    "\n",
    "train_data = list(\n",
    "    map(map_to_training_entry, raw_data.to_dict('records'))\n",
    ")\n",
    "train_data = list(filter(lambda entry: not entities_overlap(entry), train_data))\n",
    "\n",
    "train_sample, test_sample = train_test_split(\n",
    "    train_data, test_size = 0.2, random_state = 420\n",
    ")\n",
    "print('train entries: {} | test entries: {}'.format(len(train_sample), len(test_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c32154f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysaa\\anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"c/o Hitachi Research Laboratory, HITACHI LTD., 1-1...\" with entities \"[(33, 45, 'co'), (5, 31, 'building'), (69, 88, 'st...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\hysaa\\anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"10100 Bay Area Boulevard~Pasadena~Texas~77507 US\" with entities \"[(6, 24, 'street'), (0, 5, 'nr'), (40, 45, 'postal...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 | Losses: {'ner': 4065.3456535532605}\n",
      "Iteration: 1 | Losses: {'ner': 3720.6703632468416}\n",
      "Iteration: 2 | Losses: {'ner': 3326.003106552136}\n",
      "Iteration: 3 | Losses: {'ner': 3094.0743300091885}\n",
      "Iteration: 4 | Losses: {'ner': 2985.8272367392465}\n",
      "Iteration: 5 | Losses: {'ner': 2818.5104166490983}\n",
      "Iteration: 6 | Losses: {'ner': 2711.519315949423}\n",
      "Iteration: 7 | Losses: {'ner': 2604.7531162635523}\n",
      "Iteration: 8 | Losses: {'ner': 2529.309734144115}\n",
      "Iteration: 9 | Losses: {'ner': 2452.243666136069}\n",
      "Iteration: 10 | Losses: {'ner': 2407.7455834547472}\n",
      "Iteration: 11 | Losses: {'ner': 2294.013091599905}\n",
      "Iteration: 12 | Losses: {'ner': 2285.2908805757506}\n",
      "Iteration: 13 | Losses: {'ner': 2223.6780533953665}\n",
      "Iteration: 14 | Losses: {'ner': 2133.471455242254}\n",
      "Iteration: 15 | Losses: {'ner': 2049.3649079094075}\n",
      "Iteration: 16 | Losses: {'ner': 2036.24709377704}\n",
      "Iteration: 17 | Losses: {'ner': 1922.1303416627566}\n",
      "Iteration: 18 | Losses: {'ner': 1776.243404672808}\n",
      "Iteration: 19 | Losses: {'ner': 1912.3150862095458}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "for token in TOKEN_TYPES:\n",
    "    ner.add_label(token)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "for itn in range(20):\n",
    "    random.shuffle(train_sample)\n",
    "    losses = {}\n",
    "\n",
    "    batches = minibatch(train_sample, size=compounding(4, 32, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "            texts,  \n",
    "            annotations,  \n",
    "            drop=0.5,  \n",
    "            sgd=optimizer,\n",
    "            losses=losses)\n",
    "    print('Iteration: {} | Losses: {}'.format(itn, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "097a524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def results_per_entity_to_df(res: dict):\n",
    "    columns = ['Token', 'Precision', 'Recall', 'F1 score']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    total = pd.concat(\n",
    "        [pd.DataFrame([['Total', res['ents_p'], res['ents_r'], res['ents_f']]], columns=columns)]\n",
    "        , ignore_index=True\n",
    "    )\n",
    "    per_entity = pd.concat(\n",
    "        [pd.DataFrame([\n",
    "            [token, \n",
    "             res['ents_per_type'][token]['p'], \n",
    "             res['ents_per_type'][token]['r'], \n",
    "             res['ents_per_type'][token]['f']]\n",
    "        ], columns=columns) for token in TOKEN_TYPES], ignore_index=True\n",
    "    )\n",
    "    return pd.concat([per_entity, total], ignore_index=True)\n",
    "\n",
    "\n",
    "def map_to_evaluation_model(entry: tuple):\n",
    "    return (entry[0], entry[1]['entities'])\n",
    "\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot)\n",
    "        pred_value = ner_model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores\n",
    "\n",
    "train_results = evaluate(nlp, map(map_to_evaluation_model, train_sample))\n",
    "test_results = evaluate(nlp, map(map_to_evaluation_model, test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a177d08",
   "metadata": {},
   "source": [
    "With that we can conclude values on Precision, Recall and F1 score, that are quite high, considering what would be without the improvements we already made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af8f31ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- GENERAL: Results on train data ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>79.611650</td>\n",
       "      <td>73.873874</td>\n",
       "      <td>76.635514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>94.117647</td>\n",
       "      <td>48.484848</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>95.965418</td>\n",
       "      <td>94.200849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>95.170455</td>\n",
       "      <td>98.820059</td>\n",
       "      <td>96.960926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>67.741935</td>\n",
       "      <td>28.378378</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>87.241379</td>\n",
       "      <td>89.399293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>82.435597</td>\n",
       "      <td>90.256410</td>\n",
       "      <td>86.168911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>53.913043</td>\n",
       "      <td>60.194175</td>\n",
       "      <td>56.880734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>89.576547</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>90.609555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>87.474500</td>\n",
       "      <td>87.689162</td>\n",
       "      <td>87.581699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- GENERAL: Results on test data ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>39.583333</td>\n",
       "      <td>32.203390</td>\n",
       "      <td>35.514019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>72.330097</td>\n",
       "      <td>84.180791</td>\n",
       "      <td>77.806789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>94.444444</td>\n",
       "      <td>92.391304</td>\n",
       "      <td>93.406593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>83.116883</td>\n",
       "      <td>79.012346</td>\n",
       "      <td>81.012658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>68.041237</td>\n",
       "      <td>67.005076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>37.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>82.191781</td>\n",
       "      <td>77.922078</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>72.089314</td>\n",
       "      <td>73.021002</td>\n",
       "      <td>72.552167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "print('---- GENERAL: Results on train data ----')\n",
    "display(HTML(results_per_entity_to_df(train_results).to_html(index=False)))\n",
    "print('---- GENERAL: Results on test data ----')\n",
    "display(HTML(results_per_entity_to_df(test_results).to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83600ae5",
   "metadata": {},
   "source": [
    "The precision/recall of attributes postal, city, street and house number are the most important, so we'll look into optimizing them. First, let's check how many each country's addresses are there that could have influence on the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b3483b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person_ctry_code\n",
       "US    347\n",
       "JP    198\n",
       "DE     89\n",
       "FR     48\n",
       "KR     43\n",
       "GB     37\n",
       "CN     27\n",
       "TW     18\n",
       "IT     16\n",
       "CA     16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.groupby(['person_ctry_code']).size().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b52b43",
   "metadata": {},
   "source": [
    "We can also check how accurate the predictions are with each country's addresses, including both seen and unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee3ba572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correctness_by_country(country_code: str, frame: pd.DataFrame):\n",
    "    filtered = frame[frame['person_ctry_code'] == country_code]\n",
    "    mapped = list(\n",
    "        map(map_to_training_entry, filtered.to_dict('records'))\n",
    "    )\n",
    "    results = evaluate(nlp, map(map_to_evaluation_model, mapped))\n",
    "    print('---- Results on {} addresses ----'.format(country_code))\n",
    "    display(HTML(results_per_entity_to_df(results).to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1e755",
   "metadata": {},
   "source": [
    "Now we can check those with more entries, that would make remarkable changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c1a84c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Results on US addresses ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysaa\\AppData\\Local\\Temp/ipykernel_21600/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"10100 Bay Area Boulevard~Pasadena~Texas~77507 US\" with entities \"[(6, 24, 'street'), (0, 5, 'nr'), (40, 45, 'postal...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>83.783784</td>\n",
       "      <td>50.819672</td>\n",
       "      <td>63.265306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>95.364238</td>\n",
       "      <td>92.604502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>93.650794</td>\n",
       "      <td>99.159664</td>\n",
       "      <td>96.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>58.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>95.762712</td>\n",
       "      <td>93.388430</td>\n",
       "      <td>94.560669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>89.147287</td>\n",
       "      <td>94.262295</td>\n",
       "      <td>91.633466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>38.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>97.446809</td>\n",
       "      <td>98.283262</td>\n",
       "      <td>97.863248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>91.708797</td>\n",
       "      <td>91.894630</td>\n",
       "      <td>91.801619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_correctness_by_country('US', raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe088e",
   "metadata": {},
   "source": [
    "As we can see, American addresses have good results, so it's not worth it to change how we treat them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99ee71fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysaa\\AppData\\Local\\Temp/ipykernel_21600/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"50-6, Ipponmatsu, Menjyo, Yamato-cho, Ichinomiya-s...\" with entities \"[(6, 16, 'street'), (0, 4, 'nr'), (18, 24, 'area')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n",
      "C:\\Users\\hysaa\\AppData\\Local\\Temp/ipykernel_21600/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"c/o Hitachi Research Laboratory, HITACHI LTD., 1-1...\" with entities \"[(33, 45, 'co'), (5, 31, 'building'), (69, 88, 'st...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Results on JP addresses ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>65.789474</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>70.093458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>81.868132</td>\n",
       "      <td>86.627907</td>\n",
       "      <td>84.180791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>91.780822</td>\n",
       "      <td>91.780822</td>\n",
       "      <td>91.780822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>27.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>31.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>60.902256</td>\n",
       "      <td>71.052632</td>\n",
       "      <td>65.587045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>59.154930</td>\n",
       "      <td>60.869565</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>78.571429</td>\n",
       "      <td>72.131148</td>\n",
       "      <td>75.213675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>71.975498</td>\n",
       "      <td>73.208723</td>\n",
       "      <td>72.586873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_correctness_by_country('JP', raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1b7bb",
   "metadata": {},
   "source": [
    "On the other hand, Japanese addresses treatment could be better, so we hold this thought for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d5b7820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Results on DE addresses ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>76.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>96.385542</td>\n",
       "      <td>98.765432</td>\n",
       "      <td>97.560976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>96.078431</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>95.918367</td>\n",
       "      <td>94.949495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>97.916667</td>\n",
       "      <td>92.156863</td>\n",
       "      <td>94.949495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>94.331984</td>\n",
       "      <td>93.762575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_correctness_by_country('DE', raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34dfa2",
   "metadata": {},
   "source": [
    "When we get to German addresses, the results come back to top, with good Precision, Recal and F1 score. We will not check the other coutries because, since they have only a few entries (each equals less than 5% of all addresses), they would't make much of a difference on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b355578",
   "metadata": {},
   "source": [
    "## Japanese addresses\n",
    "Now we can take a look on Japanese addresses and try improving them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9eda9f",
   "metadata": {},
   "source": [
    "First, we train the model without Japanese addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68e38f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train entries: 640 | test entries: 161\n"
     ]
    }
   ],
   "source": [
    "train_data = list(\n",
    "    map(map_to_training_entry, raw_data[raw_data['person_ctry_code'] != 'JP'].to_dict('records'))\n",
    ")\n",
    "train_data = list(filter(lambda entry: not entities_overlap(entry), train_data))\n",
    "\n",
    "train_sample, test_sample = train_test_split(\n",
    "    train_data, test_size = 0.2, random_state = 420\n",
    ")\n",
    "print('train entries: {} | test entries: {}'.format(len(train_sample), len(test_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "689e4735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysaa\\anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"10100 Bay Area Boulevard~Pasadena~Texas~77507 US\" with entities \"[(6, 24, 'street'), (0, 5, 'nr'), (40, 45, 'postal...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 | Losses: {'ner': 2865.827038628515}\n",
      "Iteration: 1 | Losses: {'ner': 2340.1719870500783}\n",
      "Iteration: 2 | Losses: {'ner': 2135.6300733220473}\n",
      "Iteration: 3 | Losses: {'ner': 2020.9638930069116}\n",
      "Iteration: 4 | Losses: {'ner': 1933.9983500085134}\n",
      "Iteration: 5 | Losses: {'ner': 1838.3589959482265}\n",
      "Iteration: 6 | Losses: {'ner': 1788.712926915654}\n",
      "Iteration: 7 | Losses: {'ner': 1724.030529388439}\n",
      "Iteration: 8 | Losses: {'ner': 1726.3081249713505}\n",
      "Iteration: 9 | Losses: {'ner': 1606.4385549175456}\n",
      "Iteration: 10 | Losses: {'ner': 1601.9120288613767}\n",
      "Iteration: 11 | Losses: {'ner': 1540.5574368811526}\n",
      "Iteration: 12 | Losses: {'ner': 1500.4573989946443}\n",
      "Iteration: 13 | Losses: {'ner': 1470.2659198276144}\n",
      "Iteration: 14 | Losses: {'ner': 1392.764609863558}\n",
      "Iteration: 15 | Losses: {'ner': 1395.2874814510274}\n",
      "Iteration: 16 | Losses: {'ner': 1302.4535763420422}\n",
      "Iteration: 17 | Losses: {'ner': 1274.0767997578296}\n",
      "Iteration: 18 | Losses: {'ner': 1248.2653037697391}\n",
      "Iteration: 19 | Losses: {'ner': 1255.0249203609394}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "for token in TOKEN_TYPES:\n",
    "    ner.add_label(token)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "for itn in range(20):\n",
    "    random.shuffle(train_sample)\n",
    "    losses = {}\n",
    "\n",
    "    batches = minibatch(train_sample, size=compounding(4, 32, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "            texts,  \n",
    "            annotations,  \n",
    "            drop=0.5,  \n",
    "            sgd=optimizer,\n",
    "            losses=losses)\n",
    "    print('Iteration: {} | Losses: {}'.format(itn, losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b053a60",
   "metadata": {},
   "source": [
    "Now we can see that the overral Precision, Recall and F1 score are increased, meaning that the Japanese addresses do have impact on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f438a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysaa\\AppData\\Local\\Temp/ipykernel_21600/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"10100 Bay Area Boulevard~Pasadena~Texas~77507 US\" with entities \"[(6, 24, 'street'), (0, 5, 'nr'), (40, 45, 'postal...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Without JAPAN: Results on train data ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>78.873239</td>\n",
       "      <td>76.190476</td>\n",
       "      <td>77.508651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>80.769231</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>77.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>95.087719</td>\n",
       "      <td>96.785714</td>\n",
       "      <td>95.929204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>97.602740</td>\n",
       "      <td>98.958333</td>\n",
       "      <td>98.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>55.882353</td>\n",
       "      <td>34.545455</td>\n",
       "      <td>42.696629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>90.877193</td>\n",
       "      <td>95.220588</td>\n",
       "      <td>92.998205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>89.137380</td>\n",
       "      <td>92.079208</td>\n",
       "      <td>90.584416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>65.517241</td>\n",
       "      <td>71.698113</td>\n",
       "      <td>68.468468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>91.935484</td>\n",
       "      <td>92.307692</td>\n",
       "      <td>92.121212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>90.599593</td>\n",
       "      <td>91.295443</td>\n",
       "      <td>90.946187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Without JAPAN: Results on test data ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>38.888889</td>\n",
       "      <td>41.176471</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>28.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>80.666667</td>\n",
       "      <td>87.050360</td>\n",
       "      <td>83.737024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>93.846154</td>\n",
       "      <td>87.142857</td>\n",
       "      <td>90.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>15.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>91.304348</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>75.308642</td>\n",
       "      <td>87.142857</td>\n",
       "      <td>80.794702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>85.483871</td>\n",
       "      <td>76.811594</td>\n",
       "      <td>80.916031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>77.319588</td>\n",
       "      <td>79.957356</td>\n",
       "      <td>78.616352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = evaluate(nlp, map(map_to_evaluation_model, train_sample))\n",
    "test_results = evaluate(nlp, map(map_to_evaluation_model, test_sample))\n",
    "\n",
    "print('---- Without JAPAN: Results on train data ----')\n",
    "display(HTML(results_per_entity_to_df(train_results).to_html(index=False)))\n",
    "print('---- Without JAPAN: Results on test data ----')\n",
    "display(HTML(results_per_entity_to_df(test_results).to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb77ad7",
   "metadata": {},
   "source": [
    "To prove that, when we check about the Precision, Recall and F1 score on Japanese addresses, we can see that they are quite low, lower than when we trained all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "146d1bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysaa\\AppData\\Local\\Temp/ipykernel_21600/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"50-6, Ipponmatsu, Menjyo, Yamato-cho, Ichinomiya-s...\" with entities \"[(6, 16, 'street'), (0, 4, 'nr'), (18, 24, 'area')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n",
      "C:\\Users\\hysaa\\AppData\\Local\\Temp/ipykernel_21600/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"c/o Hitachi Research Laboratory, HITACHI LTD., 1-1...\" with entities \"[(33, 45, 'co'), (5, 31, 'building'), (69, 88, 'st...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Results on JP addresses ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>37.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>62.790698</td>\n",
       "      <td>59.833795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>74.117647</td>\n",
       "      <td>86.301370</td>\n",
       "      <td>79.746835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>23.913043</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>36.065574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>22.727273</td>\n",
       "      <td>13.157895</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>43.548387</td>\n",
       "      <td>39.130435</td>\n",
       "      <td>41.221374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>42.814371</td>\n",
       "      <td>44.548287</td>\n",
       "      <td>43.664122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_correctness_by_country('JP', raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e3dbb",
   "metadata": {},
   "source": [
    "Now it's time for we to train only Japanese addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "126e347e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train entries: 158 | test entries: 40\n"
     ]
    }
   ],
   "source": [
    "train_data = list(\n",
    "    map(map_to_training_entry, raw_data[raw_data['person_ctry_code'] == 'JP'].to_dict('records'))\n",
    ")\n",
    "train_data = list(filter(lambda entry: not entities_overlap(entry), train_data))\n",
    "\n",
    "train_sample, test_sample = train_test_split(\n",
    "    train_data, test_size = 0.2, random_state = 420\n",
    ")\n",
    "print('train entries: {} | test entries: {}'.format(len(train_sample), len(test_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e367a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysaa\\anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"50-6, Ipponmatsu, Menjyo, Yamato-cho, Ichinomiya-s...\" with entities \"[(6, 16, 'street'), (0, 4, 'nr'), (18, 24, 'area')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\hysaa\\anaconda3\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"c/o Hitachi Research Laboratory, HITACHI LTD., 1-1...\" with entities \"[(33, 45, 'co'), (5, 31, 'building'), (69, 88, 'st...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 | Losses: {'ner': 1284.1569767929614}\n",
      "Iteration: 1 | Losses: {'ner': 1078.0679613405373}\n",
      "Iteration: 2 | Losses: {'ner': 1175.3667421340942}\n",
      "Iteration: 3 | Losses: {'ner': 1129.9756172895432}\n",
      "Iteration: 4 | Losses: {'ner': 1055.992091074586}\n",
      "Iteration: 5 | Losses: {'ner': 1000.0957781840116}\n",
      "Iteration: 6 | Losses: {'ner': 954.5107497198042}\n",
      "Iteration: 7 | Losses: {'ner': 909.0037981718779}\n",
      "Iteration: 8 | Losses: {'ner': 937.0342071205378}\n",
      "Iteration: 9 | Losses: {'ner': 858.214796923101}\n",
      "Iteration: 10 | Losses: {'ner': 861.7343583619222}\n",
      "Iteration: 11 | Losses: {'ner': 813.293468308635}\n",
      "Iteration: 12 | Losses: {'ner': 768.2377245387615}\n",
      "Iteration: 13 | Losses: {'ner': 795.7788266167045}\n",
      "Iteration: 14 | Losses: {'ner': 824.7075342992321}\n",
      "Iteration: 15 | Losses: {'ner': 761.737893326208}\n",
      "Iteration: 16 | Losses: {'ner': 687.36905702285}\n",
      "Iteration: 17 | Losses: {'ner': 785.5316795790568}\n",
      "Iteration: 18 | Losses: {'ner': 676.8775275740772}\n",
      "Iteration: 19 | Losses: {'ner': 683.3151769563556}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "for token in TOKEN_TYPES:\n",
    "    ner.add_label(token)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "for itn in range(20):\n",
    "    random.shuffle(train_sample)\n",
    "    losses = {}\n",
    "\n",
    "    batches = minibatch(train_sample, size=compounding(4, 32, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "            texts,  \n",
    "            annotations,  \n",
    "            drop=0.5,  \n",
    "            sgd=optimizer,\n",
    "            losses=losses)\n",
    "    print('Iteration: {} | Losses: {}'.format(itn, losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1225558",
   "metadata": {},
   "source": [
    "And as we were expecting, the Precision, Recall and F1 score are increased when they are trained separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b47ab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hysaa\\AppData\\Local\\Temp/ipykernel_21600/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"50-6, Ipponmatsu, Menjyo, Yamato-cho, Ichinomiya-s...\" with entities \"[(6, 16, 'street'), (0, 4, 'nr'), (18, 24, 'area')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n",
      "C:\\Users\\hysaa\\AppData\\Local\\Temp/ipykernel_21600/689442182.py:30: UserWarning: [W030] Some entities could not be aligned in the text \"c/o Hitachi Research Laboratory, HITACHI LTD., 1-1...\" with entities \"[(33, 45, 'co'), (5, 31, 'building'), (69, 88, 'st...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities (with BILUO tag '-') will be ignored during training.\n",
      "  gold = GoldParse(doc_gold_text, entities=annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Results on JP addresses ----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Token</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>area</td>\n",
       "      <td>64.761905</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>66.341463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>country</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>city</td>\n",
       "      <td>77.083333</td>\n",
       "      <td>86.046512</td>\n",
       "      <td>81.318681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>postal</td>\n",
       "      <td>89.610390</td>\n",
       "      <td>94.520548</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>building</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>42.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nr</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>20.512821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>street</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>44.736842</td>\n",
       "      <td>47.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>co</td>\n",
       "      <td>51.351351</td>\n",
       "      <td>55.072464</td>\n",
       "      <td>53.146853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>region</td>\n",
       "      <td>75.925926</td>\n",
       "      <td>67.213115</td>\n",
       "      <td>71.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total</td>\n",
       "      <td>68.539326</td>\n",
       "      <td>66.510903</td>\n",
       "      <td>67.509881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_correctness_by_country('JP', raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b987f7",
   "metadata": {},
   "source": [
    "## Why not trying new libraries\n",
    "There are several libraries where NLP models can be done, among them Stanza library. The reason why we did not implement them is because, since we already have a pretty good model and Stanza is a whole new library, mostly recommended to identification of word types, such as pronouns, verbs and names, we found changing it all an unnecessary work. With that said, our improvement is mostly for Japanese addresses, that are a lot, and it really improved our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50c70c",
   "metadata": {},
   "source": [
    "# Final solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf780051",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a133baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "from utils import read_DataFrame_from_excel, resolve_model_name\n",
    "from spacy.util import compounding, minibatch\n",
    "\n",
    "\n",
    "TRAINING_DATA_FILENAME = './files/training_data_fixed.xlsx'\n",
    "TRAINING_ENTRIES_COUNT = 999\n",
    "TRAINED_MODEL_FILENAME = './models/trained_model'\n",
    "\n",
    "TOKEN_TYPES: set = {'co', 'building', 'street', 'nr', 'area', 'postal', 'city', 'region', 'country'}\n",
    "\n",
    "TRAIN_ITERATION_COUNT = 20\n",
    "TRAIN_DROP_PROPERTY = 0.5\n",
    "\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Performs data preprocessing by adding a space after each comma and semicolon if they are missing\n",
    "    \n",
    "    Args:\n",
    "        dataFrame (pd.DataFrame): dataset to be processed\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for col in data.columns:\n",
    "        data[col] = data.apply(lambda row: re.sub(r'([^\\s])([,;])([^\\s])', r'\\1\\2 \\3', str(row[col])), axis=1)\n",
    "\n",
    "\n",
    "def get_entity_list(entry: dict, adr: str):\n",
    "    \"\"\"\n",
    "    Extracts an array of tuples, indicating positions of tokens in a provided address\n",
    "    \n",
    "    Args:\n",
    "        entry (dict): dictionary, where keys are token types.\n",
    "            Example:\n",
    "            dict = {\n",
    "                'city': 'Vilnius',\n",
    "                'street': 'Ozo g.',\n",
    "                'nr': 25\n",
    "            }\n",
    "        adr (str): an address string.\n",
    "            Example: \n",
    "            adr = 'Ozo g. 25, Vilnius'\n",
    "\n",
    "    Returns:\n",
    "        Array of tuples, where tuples follow structure of (token_position_start, token_position_end, token)\n",
    "    \"\"\"\n",
    "    address = str(adr)\n",
    "    entities: list = []\n",
    "    present_tokens = filter(lambda item: item[0] in TOKEN_TYPES and item[1] and str(item[1]).strip(), entry.items())\n",
    "\n",
    "    ## tokens to retry matching\n",
    "    retry_tokens: set = set()\n",
    "\n",
    "    for item in present_tokens:\n",
    "        token_value = str(item[1]).strip()\n",
    "        match = re.search(re.escape(token_value), address)\n",
    "        if match:\n",
    "            # If multiple occurences can be matched, save the token to be matched later\n",
    "            if (len(re.findall(re.escape(token_value), address)) > 1):\n",
    "                retry_tokens.add((token_value, item[0]))\n",
    "                continue\n",
    "            span = match.span()\n",
    "            entities.append((span[0], span[1], item[0]))\n",
    "            # Replace matched entity with symbols, so that parts of it cannot be matched again\n",
    "            address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "        else:\n",
    "            # Try and resolve multiple tokens separated by ';'\n",
    "            split_items = map(lambda token: token.strip(), token_value.split(';'))\n",
    "            for token in split_items:\n",
    "                split_match = re.search(re.escape(token), address)\n",
    "                if split_match:\n",
    "                    # If multiple occurences can be matched, save the token to be matched later\n",
    "                    if (len(re.findall(re.escape(token), address)) > 1):\n",
    "                        retry_tokens.add((token, item[0]))\n",
    "                        continue\n",
    "                    span = split_match.span()\n",
    "                    entities.append((span[0], span[1], item[0]))\n",
    "                    # Replace matched entity with symbols, so that parts of it cannot be matched again\n",
    "                    address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "                else:\n",
    "                    print('WARNING: could not find token \"{}\" in address \"{}\"'.format(token, adr))\n",
    "    \n",
    "    # Try and match previously marked tokens, now that single-match entities were eliminated\n",
    "    for token, tkn_type in retry_tokens:\n",
    "        token_value = str(token).strip()\n",
    "        match = re.search(re.escape(token_value), address)\n",
    "        if match:\n",
    "            span = match.span()\n",
    "            entities.append((span[0], span[1], tkn_type))\n",
    "            address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "        else:\n",
    "            print('WARNING: could not find token \"{}\" in address \"{}\"'.format(token, adr))\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def map_to_training_entry(entry: dict):\n",
    "    \"\"\"\n",
    "    Maps an object of address tokens into a tuple of address string and an object containing entity list.\n",
    "    \n",
    "    Args:\n",
    "        entry (dict): dictionary, where keys include token types.\n",
    "            Example:\n",
    "            dict = {\n",
    "                'person_address': 'Ozo g. 25, Vilnius',\n",
    "                'city': 'Vilnius',\n",
    "                'street': 'Ozo g.',\n",
    "                'nr': 25\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        A tuple, where first element is the address, and the second one is an object containing the entity list\n",
    "    \"\"\"\n",
    "    address = entry['person_address']\n",
    "    return (address, {\n",
    "        'entities': get_entity_list(entry, address)\n",
    "    })\n",
    "\n",
    "\n",
    "def entities_overlap(entry):\n",
    "    \"\"\"\n",
    "    Checks whether an entry contains overlapping entities\n",
    "    \n",
    "    Args:\n",
    "        entry (array or tuple): dictionary, where keys are token types.\n",
    "            Example:\n",
    "            dict = {\n",
    "                'city': 'Vilnius',\n",
    "                'street': 'Ozo g.',\n",
    "                'nr': 25\n",
    "            }\n",
    "        adr (str): an address string.\n",
    "            Example: \n",
    "            adr = 'Ozo g. 25, Vilnius'\n",
    "\n",
    "    Returns:\n",
    "        Array of tuples, where tuples follow structure of (token_position_start, token_position_end, token)\n",
    "    \"\"\"\n",
    "    entities = entry[1]['entities']\n",
    "    for first in entities:\n",
    "        for second in entities:\n",
    "            if (first == second): continue\n",
    "            if (first[0] < second[0] and first[1] > second[0]) or (first[0] > second[0] and first[1] < second[0]) or (first[0]==second[0] or first[1]==second[1]):\n",
    "                print('Entities {} and {} overlap in \"{}\"'.format(first, second, entry[0]))\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def train_model(entries: pd.DataFrame, model_filename: str):\n",
    "    \"\"\"\n",
    "    Train a NER model from a given input dataframe and saves the model to disk.\n",
    "\n",
    "    Args:\n",
    "        entries - a pandas DataFrame containing the training data\n",
    "        model_filename - where on disk to output the model\n",
    "    \"\"\"\n",
    "    train_data = map(map_to_training_entry, entries.to_dict('records'))\n",
    "    train_data = list(filter(lambda entry: not entities_overlap(entry), train_data))\n",
    "\n",
    "    nlp = spacy.blank('en')\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "\n",
    "    for token in TOKEN_TYPES:\n",
    "        ner.add_label(token)\n",
    "    \n",
    "    print('--- TRAINING {} MODEL IN {} ITERATIONS | DROP = {} ---'.format(model_filename, TRAIN_ITERATION_COUNT, TRAIN_DROP_PROPERTY))\n",
    "    print('--- TRAIN DATA SIZE: {} ---'.format(len(entries)))\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    for itn in range(TRAIN_ITERATION_COUNT):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "\n",
    "        batches = minibatch(train_data, size=compounding(4, 32, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  \n",
    "                annotations,  \n",
    "                drop=TRAIN_DROP_PROPERTY,  \n",
    "                sgd=optimizer,\n",
    "                losses=losses)\n",
    "        print('Iteration: {} | Losses: {}'.format(itn, losses))\n",
    "\n",
    "    nlp.to_disk(model_filename)\n",
    "\n",
    "\n",
    "def train_model_without_countries(raw_data: pd.DataFrame, exclude: list):\n",
    "    train_model(raw_data[~raw_data['person_ctry_code'].isin(exclude)], resolve_model_name())\n",
    "\n",
    "\n",
    "def train_model_for_country(raw_data: pd.DataFrame, country_code: str):\n",
    "    train_model(raw_data[raw_data['person_ctry_code'] == country_code], resolve_model_name(country_code))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    raw_data: pd.DataFrame = read_DataFrame_from_excel(TRAINING_DATA_FILENAME, TRAINING_ENTRIES_COUNT)\n",
    "    preprocess_data(raw_data)\n",
    "\n",
    "    train_model_for_country(raw_data, 'JP')\n",
    "    train_model_without_countries(raw_data, ['JP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10294298",
   "metadata": {},
   "source": [
    "## deploy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2dbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "from utils import read_DataFrame_from_excel, resolve_model_name\n",
    "from spacy.util import compounding, minibatch\n",
    "\n",
    "\n",
    "TRAINING_DATA_FILENAME = './files/training_data_fixed.xlsx'\n",
    "TRAINING_ENTRIES_COUNT = 999\n",
    "TRAINED_MODEL_FILENAME = './models/trained_model'\n",
    "\n",
    "TOKEN_TYPES: set = {'co', 'building', 'street', 'nr', 'area', 'postal', 'city', 'region', 'country'}\n",
    "\n",
    "TRAIN_ITERATION_COUNT = 20\n",
    "TRAIN_DROP_PROPERTY = 0.5\n",
    "\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Performs data preprocessing by adding a space after each comma and semicolon if they are missing\n",
    "    \n",
    "    Args:\n",
    "        dataFrame (pd.DataFrame): dataset to be processed\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for col in data.columns:\n",
    "        data[col] = data.apply(lambda row: re.sub(r'([^\\s])([,;])([^\\s])', r'\\1\\2 \\3', str(row[col])), axis=1)\n",
    "\n",
    "\n",
    "def get_entity_list(entry: dict, adr: str):\n",
    "    \"\"\"\n",
    "    Extracts an array of tuples, indicating positions of tokens in a provided address\n",
    "    \n",
    "    Args:\n",
    "        entry (dict): dictionary, where keys are token types.\n",
    "            Example:\n",
    "            dict = {\n",
    "                'city': 'Vilnius',\n",
    "                'street': 'Ozo g.',\n",
    "                'nr': 25\n",
    "            }\n",
    "        adr (str): an address string.\n",
    "            Example: \n",
    "            adr = 'Ozo g. 25, Vilnius'\n",
    "\n",
    "    Returns:\n",
    "        Array of tuples, where tuples follow structure of (token_position_start, token_position_end, token)\n",
    "    \"\"\"\n",
    "    address = str(adr)\n",
    "    entities: list = []\n",
    "    present_tokens = filter(lambda item: item[0] in TOKEN_TYPES and item[1] and str(item[1]).strip(), entry.items())\n",
    "\n",
    "    ## tokens to retry matching\n",
    "    retry_tokens: set = set()\n",
    "\n",
    "    for item in present_tokens:\n",
    "        token_value = str(item[1]).strip()\n",
    "        match = re.search(re.escape(token_value), address)\n",
    "        if match:\n",
    "            # If multiple occurences can be matched, save the token to be matched later\n",
    "            if (len(re.findall(re.escape(token_value), address)) > 1):\n",
    "                retry_tokens.add((token_value, item[0]))\n",
    "                continue\n",
    "            span = match.span()\n",
    "            entities.append((span[0], span[1], item[0]))\n",
    "            # Replace matched entity with symbols, so that parts of it cannot be matched again\n",
    "            address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "        else:\n",
    "            # Try and resolve multiple tokens separated by ';'\n",
    "            split_items = map(lambda token: token.strip(), token_value.split(';'))\n",
    "            for token in split_items:\n",
    "                split_match = re.search(re.escape(token), address)\n",
    "                if split_match:\n",
    "                    # If multiple occurences can be matched, save the token to be matched later\n",
    "                    if (len(re.findall(re.escape(token), address)) > 1):\n",
    "                        retry_tokens.add((token, item[0]))\n",
    "                        continue\n",
    "                    span = split_match.span()\n",
    "                    entities.append((span[0], span[1], item[0]))\n",
    "                    # Replace matched entity with symbols, so that parts of it cannot be matched again\n",
    "                    address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "                else:\n",
    "                    print('WARNING: could not find token \"{}\" in address \"{}\"'.format(token, adr))\n",
    "    \n",
    "    # Try and match previously marked tokens, now that single-match entities were eliminated\n",
    "    for token, tkn_type in retry_tokens:\n",
    "        token_value = str(token).strip()\n",
    "        match = re.search(re.escape(token_value), address)\n",
    "        if match:\n",
    "            span = match.span()\n",
    "            entities.append((span[0], span[1], tkn_type))\n",
    "            address = address[:span[0]] + '$' * (span[1] - span[0]) + address[span[1]:]\n",
    "        else:\n",
    "            print('WARNING: could not find token \"{}\" in address \"{}\"'.format(token, adr))\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def map_to_training_entry(entry: dict):\n",
    "    \"\"\"\n",
    "    Maps an object of address tokens into a tuple of address string and an object containing entity list.\n",
    "    \n",
    "    Args:\n",
    "        entry (dict): dictionary, where keys include token types.\n",
    "            Example:\n",
    "            dict = {\n",
    "                'person_address': 'Ozo g. 25, Vilnius',\n",
    "                'city': 'Vilnius',\n",
    "                'street': 'Ozo g.',\n",
    "                'nr': 25\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        A tuple, where first element is the address, and the second one is an object containing the entity list\n",
    "    \"\"\"\n",
    "    address = entry['person_address']\n",
    "    return (address, {\n",
    "        'entities': get_entity_list(entry, address)\n",
    "    })\n",
    "\n",
    "\n",
    "def entities_overlap(entry):\n",
    "    \"\"\"\n",
    "    Checks whether an entry contains overlapping entities\n",
    "    \n",
    "    Args:\n",
    "        entry (array or tuple): dictionary, where keys are token types.\n",
    "            Example:\n",
    "            dict = {\n",
    "                'city': 'Vilnius',\n",
    "                'street': 'Ozo g.',\n",
    "                'nr': 25\n",
    "            }\n",
    "        adr (str): an address string.\n",
    "            Example: \n",
    "            adr = 'Ozo g. 25, Vilnius'\n",
    "\n",
    "    Returns:\n",
    "        Array of tuples, where tuples follow structure of (token_position_start, token_position_end, token)\n",
    "    \"\"\"\n",
    "    entities = entry[1]['entities']\n",
    "    for first in entities:\n",
    "        for second in entities:\n",
    "            if (first == second): continue\n",
    "            if (first[0] < second[0] and first[1] > second[0]) or (first[0] > second[0] and first[1] < second[0]) or (first[0]==second[0] or first[1]==second[1]):\n",
    "                print('Entities {} and {} overlap in \"{}\"'.format(first, second, entry[0]))\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def train_model(entries: pd.DataFrame, model_filename: str):\n",
    "    \"\"\"\n",
    "    Train a NER model from a given input dataframe and saves the model to disk.\n",
    "\n",
    "    Args:\n",
    "        entries - a pandas DataFrame containing the training data\n",
    "        model_filename - where on disk to output the model\n",
    "    \"\"\"\n",
    "    train_data = map(map_to_training_entry, entries.to_dict('records'))\n",
    "    train_data = list(filter(lambda entry: not entities_overlap(entry), train_data))\n",
    "\n",
    "    nlp = spacy.blank('en')\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "\n",
    "    for token in TOKEN_TYPES:\n",
    "        ner.add_label(token)\n",
    "    \n",
    "    print('--- TRAINING {} MODEL IN {} ITERATIONS | DROP = {} ---'.format(model_filename, TRAIN_ITERATION_COUNT, TRAIN_DROP_PROPERTY))\n",
    "    print('--- TRAIN DATA SIZE: {} ---'.format(len(entries)))\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    for itn in range(TRAIN_ITERATION_COUNT):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "\n",
    "        batches = minibatch(train_data, size=compounding(4, 32, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  \n",
    "                annotations,  \n",
    "                drop=TRAIN_DROP_PROPERTY,  \n",
    "                sgd=optimizer,\n",
    "                losses=losses)\n",
    "        print('Iteration: {} | Losses: {}'.format(itn, losses))\n",
    "\n",
    "    nlp.to_disk(model_filename)\n",
    "\n",
    "\n",
    "def train_model_without_countries(raw_data: pd.DataFrame, exclude: list):\n",
    "    train_model(raw_data[~raw_data['person_ctry_code'].isin(exclude)], resolve_model_name())\n",
    "\n",
    "\n",
    "def train_model_for_country(raw_data: pd.DataFrame, country_code: str):\n",
    "    train_model(raw_data[raw_data['person_ctry_code'] == country_code], resolve_model_name(country_code))\n",
    "\n",
    "\n",
    "# Version 2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    raw_data: pd.DataFrame = read_DataFrame_from_excel(TRAINING_DATA_FILENAME, TRAINING_ENTRIES_COUNT)\n",
    "    preprocess_data(raw_data)\n",
    "\n",
    "    train_model_for_country(raw_data, 'JP')\n",
    "    train_model_without_countries(raw_data, ['JP'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
